(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{240:function(t,e,n){"use strict";n.r(e);var r=[function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("div",{staticClass:"wrapper-small my-5"},[r("div",{staticClass:"project-card md:flex mt-8"},[r("div",{staticClass:"img max-w-lg md:max-w-sm  mx-auto"},[r("img",{staticClass:"rounded-xl",attrs:{src:n(241),alt:""}})]),t._v(" "),r("div",{staticClass:"flex flex-col justify-between max-w-lg mx-auto"},[r("div",{staticClass:"txt md:px-5 lg:px-0"},[r("h2",{staticClass:"text-xl font-semibold text-gray-800"},[t._v("\n          Multi-agent Reinforcement Learning in OpenSpiel\n        ")]),t._v(" "),r("h3",{staticClass:"text-sm lg:text-sm text-gray-500"},[t._v("\n          Michael Walton, Viliam Lisy • 2021 arXiv\n        ")]),t._v(" "),r("p",{staticClass:"text-base text-gray-700 my-3"},[t._v("\n          In this report, we present results reproductions for several core\n          algorithms implemented in the OpenSpiel framework for learning in\n          games. The primary contribution of this work is a validation of\n          OpenSpiel’s re-implemented search and Reinforcement Learning\n          algorithms against the results reported in their respective\n          originating works. Additionally, we provide complete documentation\n          of hyperparameters and source code required to reproduce these\n          experiments easily and exactly.\n        ")])]),t._v(" "),r("div",[r("div",{staticClass:"md:px-5 lg:px-0 py-3"},[r("a",{staticClass:"bg-indigo-600 hover:bg-indigo-700 px-3 py-2 text-sm text-gray-200 font-semibold font-medium rounded-full",attrs:{href:"https://arxiv.org/abs/2103.00187",target:"_blank"}},[t._v("Paper")]),t._v(" "),r("a",{staticClass:"ml-6 text-indigo-600 hover:underline",attrs:{href:"https://github.com/aicenter/openspiel_reproductions",target:"_blank"}},[t._v("Code")])])])])]),t._v(" "),r("div",{staticClass:"project-card md:flex mt-16"},[r("div",{staticClass:"img max-w-lg md:max-w-sm  mx-auto"},[r("img",{staticClass:"rounded-xl",attrs:{src:n(242),alt:""}})]),t._v(" "),r("div",{staticClass:"flex flex-col justify-between max-w-lg mx-auto"},[r("div",{staticClass:"txt md:px-5 lg:px-0"},[r("h2",{staticClass:"text-xl font-semibold text-gray-800"},[t._v("\n          Theory of Mind for Deep Reinforcement Learning in Hanabi\n        ")]),t._v(" "),r("h3",{staticClass:"text-sm lg:text-sm text-gray-500"},[t._v("\n          Andrew Fuchs, Michael Walton, Theresa Chadwick, Doug Lange • 2019\n          NeurIPS Deep Reinforcement Learning Workshop\n        ")]),t._v(" "),r("p",{staticClass:"text-base text-gray-700 my-3"},[t._v("\n          The partially observable card game Hanabi has recently been proposed\n          as a new AI challenge problem due to its dependence on implicit\n          communication conventions and apparent necessity of theory of mind\n          reasoning for efficient play. In this work, we propose a mechanism\n          for imbuing Reinforcement Learning agents with a theory of mind to\n          discover efficient cooperative strategies in Hanabi. The primary\n          contributions of this work are threefold: First, a formal definition\n          of a computationally tractable mechanism for computing hand\n          probabilities in Hanabi. Second, an extension to conventional Deep\n          Reinforcement Learning that introduces reasoning over finitely\n          nested theory of mind belief hierarchies. Finally, an intrinsic\n          reward mechanism enabled by theory of mind that incentivizes agents\n          to share strategically relevant private knowledge with their\n          teammates. We demonstrate the utility of our algorithm against\n          Rainbow, a state-of-the-art Reinforcement Learning agent.\n        ")])]),t._v(" "),r("div",[r("div",{staticClass:"md:px-5 lg:px-0 py-3"},[r("a",{staticClass:"bg-indigo-600 hover:bg-indigo-700 px-3 py-2 text-sm text-gray-200 font-semibold rounded-full",attrs:{href:"https://arxiv.org/abs/2101.09328",target:"_blank"}},[t._v("Paper")]),t._v(" "),r("a",{staticClass:"ml-6 text-indigo-600 hover:underline",attrs:{href:"https://github.com/mwalton/ToM-hanabi-neurips19",target:"_blank"}},[t._v("Code")])])])])]),t._v(" "),r("div",{staticClass:"project-card md:flex mt-16"},[r("div",{staticClass:"img max-w-lg md:max-w-sm  mx-auto"},[r("img",{staticClass:"rounded-xl",attrs:{src:n(243),alt:""}})]),t._v(" "),r("div",{staticClass:"flex flex-col justify-between max-w-lg mx-auto"},[r("div",{staticClass:"txt md:px-5 lg:px-0"},[r("h2",{staticClass:"text-xl font-semibold text-gray-800"},[t._v("\n          Unsupervised Anomaly Detection for Digital Radio Frequency\n          Transmissions\n        ")]),t._v(" "),r("h3",{staticClass:"text-sm lg:text-sm text-gray-500"},[t._v("\n          Michael Walton, Maurice Ayache, Logan Straatemeier, Daniel Gebhardt,\n          Benjamin Migliori • 2017 IEEE International Conference on Machine\n          Learning and Applications\n        ")]),t._v(" "),r("p",{staticClass:"text-base text-gray-700 my-3"},[t._v("\n          We present a novel method of unsupervised anomaly detection using\n          long-short-term memory mixture density networks (LSTM-MDN), applied\n          to timeseries data of digital radio transmissions. The modern radio\n          frequency (RF) environment is a dynamic and ever-changing complex\n          milieu of signals, environmental effects, unintentional\n          interference, and intentional jamming. A consequence of this complex\n          mix is that RF receivers must become better and better at rejecting\n          anomalous signals in order to recover the transmitted information.\n          However, it is not always possible to know a priori what constitutes\n          a valid signal and what constitutes an anomaly (intentional or\n          otherwise), especially with the adoption of cognitive radio\n          techniques. We show that an LSTM-MDN model is able to rapidly learn\n          the training set and produce probability distribution functions for\n          the expected signal as a function of time. We then demonstrate that\n          the negative log likelihood of an incoming test transmission,\n          conditioned on the training set, provides a metric that allows\n          anomalous signals to be detected and labeled. We demonstrate this\n          method for eight popular modulations and for three different anomaly\n          types. By applying unsupervised learning in the temporal domain, we\n          report a fully-generalizable anomaly detection method that may be\n          applied to signals for which the transmission parameters may be\n          unknown or obscured.\n        ")])]),t._v(" "),r("div",[r("div",{staticClass:"md:px-5 lg:px-0 py-3"},[r("a",{staticClass:"bg-indigo-600 hover:bg-indigo-700 px-3 py-2 text-sm text-gray-200 font-semibold rounded-full",attrs:{href:"https://ieeexplore.ieee.org/abstract/document/8260738",target:"_blank"}},[t._v("Paper")])])])])]),t._v(" "),r("div",{staticClass:"project-card md:flex mt-16"},[r("div",{staticClass:"img max-w-lg md:max-w-sm  mx-auto"},[r("img",{staticClass:"rounded-xl",attrs:{src:n(244),alt:""}})]),t._v(" "),r("div",{staticClass:"flex flex-col justify-between max-w-lg mx-auto"},[r("div",{staticClass:"txt md:px-5 lg:px-0"},[r("h2",{staticClass:"text-xl font-semibold text-gray-800"},[t._v("\n          Hunting for Naval Mines with Deep Neural Networks\n        ")]),t._v(" "),r("h3",{staticClass:"text-sm lg:text-sm text-gray-500"},[t._v("\n          Daniel Gebhardt, Keyur Parikh, Iryna Dzieciuch, Michael Walton, Nhut\n          Anh Vo Hoang • 2017 IEEE OCEANS\n        ")]),t._v(" "),r("p",{staticClass:"text-base text-gray-700 my-3"},[t._v("\n          Explosive naval mines pose a threat to ocean and sea faring vessels,\n          both military and civilian. This work applies deep neural network\n          (DNN) methods to the problem of detecting minelike objects (MLO) on\n          the seafloor in side-scan sonar imagery. We explored how the DNN\n          depth, memory requirements, calculation requirements, and training\n          data distribution affect detection efficacy. A visualization\n          technique (class activation map) was incorporated that aids a user\n          in interpreting the model’s behavior. We found that modest DNN model\n          sizes yielded better accuracy (98%) than very simple DNN models\n          (93%) and a support vector machine (78%). The largest DNN models\n          achieved less than 1% efficacy increase at a cost of a 17x increase\n          of trainable parameter count and computation requirements. In\n          contrast to DNNs popularized for many-class image recognition tasks,\n          the models for this task require far fewer computational resources\n          (0.3% of parameters), and are suitable for embedded use within an\n          autonomous unmanned underwater vehicle.\n        ")])]),t._v(" "),r("div",[r("div",{staticClass:"md:px-5 lg:px-0 py-3"},[r("a",{staticClass:"bg-indigo-600 hover:bg-indigo-700 px-3 py-2 text-sm text-gray-200 font-semibold rounded-full",attrs:{href:"https://ieeexplore.ieee.org/abstract/document/8232216",target:"_blank"}},[t._v("Paper")])])])])])])}],o={},l=n(11),component=Object(l.a)(o,(function(){var t=this.$createElement;this._self._c;return this._m(0)}),r,!1,null,null,null);e.default=component.exports},241:function(t,e,n){t.exports=n.p+"img/rcfr_horz.69a1624.png"},242:function(t,e,n){t.exports=n.p+"img/hanabi_horz.5595311.png"},243:function(t,e,n){t.exports=n.p+"img/rf.c520da2.png"},244:function(t,e,n){t.exports=n.p+"img/mlo.41931ad.png"},246:function(t,e,n){var content=n(256);"string"==typeof content&&(content=[[t.i,content,""]]),content.locals&&(t.exports=content.locals);(0,n(26).default)("2da826c0",content,!0,{sourceMap:!1})},249:function(t,e,n){"use strict";n.r(e);var r=[function(){var t=this.$createElement,e=this._self._c||t;return e("div",{staticClass:"text-center text-gray-700 mb-8"},[e("div",{staticClass:"pt-16 pb-4"},[e("img",{staticClass:"rounded-full mx-auto object-center",attrs:{src:n(254),width:"200",alt:"mwalton"}})]),this._v(" "),e("h4",{staticClass:"text-4xl md:text-4xl lg:text-4xl font-semibold text-gray-800"},[this._v("\n    Mike Walton\n  ")]),this._v(" "),e("h3",{staticClass:"text-lg lg:text-xl text-gray-800"},[this._v("\n    PhD Student @ CTU AI Center\n  ")]),this._v(" "),e("div",{staticClass:"wrapper text-center pt-5"},[e("h2",{staticClass:"text-lg lg:text-xl text-gray-500"},[this._v("\n      ai • reinforcement learning • game theory\n    ")])])])}],o={},l=n(11),component=Object(l.a)(o,(function(){var t=this.$createElement;this._self._c;return this._m(0)}),r,!1,null,null,null);e.default=component.exports},250:function(t,e,n){"use strict";n.r(e);var r={},o=(n(255),n(11)),component=Object(o.a)(r,(function(){var t=this.$createElement,e=this._self._c||t;return e("div",{staticClass:"wrapper text-center text-gray-700 mb-8"},[this._m(0),this._v(" "),this._m(1),this._v(" "),e("nuxt-link",{staticClass:"social-link hover:text-primary",attrs:{to:"blog"}},[e("i",{staticClass:"bx bxs-pencil"}),this._v(" Notes\n  ")])],1)}),[function(){var t=this.$createElement,e=this._self._c||t;return e("a",{staticClass:"social-link hover:text-primary",attrs:{href:"https://github.com/mwalton",target:"_blank","aria-label":"Github"}},[e("i",{staticClass:"bx bxl-github"}),this._v(" Code\n  ")])},function(){var t=this.$createElement,e=this._self._c||t;return e("a",{staticClass:"social-link hover:text-primary",attrs:{href:"https://scholar.google.com/citations?user=TTEHCqUAAAAJ&hl=en",target:"_blank","aria-label":"Scholar"}},[e("i",{staticClass:"bx bxs-graduation"}),this._v(" Papers\n  ")])}],!1,null,null,null);e.default=component.exports},251:function(t,e,n){"use strict";n.r(e);n(33);var r=n(5),o={components:{},data:function(){return{posts:{}}},fetch:function(){var t=this;return Object(r.a)(regeneratorRuntime.mark((function e(){return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return e.next=2,t.$content("affiliations").sortBy("order").fetch();case 2:t.posts=e.sent;case 3:case"end":return e.stop()}}),e)})))()}},l=n(11),component=Object(l.a)(o,(function(){var t=this,e=t.$createElement,n=t._self._c||e;return n("div",{staticClass:"wrapper-small"},t._l(t.posts,(function(e){return n("div",{key:e.slug,staticClass:"my-10 bg-gray-100 p-4 md:p-6 rounded-xl shadow-md"},[n("div",{staticClass:"mt-2"},[n("div",{staticClass:"flex flex-col md:flex-row md:space-x-10 items-center"},[n("a",{staticClass:"text-md md:text-2xl text-gray-700 font-bold hover:text-gray-600 hover:underline",attrs:{target:"_blank",href:e.url}},[n("img",{staticClass:"rounded-md w-64 md:w-32",attrs:{src:e.img,alt:"derp"}})]),t._v(" "),n("div",[n("a",{staticClass:"text-md md:text-2xl text-gray-700 font-bold hover:text-gray-600 hover:underline",attrs:{target:"_blank",href:e.url}},[t._v("\n            "+t._s(e.organization)+"\n          ")]),t._v(" "),n("p",{staticClass:"mt-2 text-base text-md md:text-lg text-gray-600"},[t._v("\n            "+t._s(e.position)+" • "+t._s(e.time)+"\n          ")]),t._v(" "),n("p",{staticClass:"mt-2 text-base text-md md:text-lg text-gray-600"},[t._v("\n            "+t._s(e.location)+"\n          ")])])])])])})),0)}),[],!1,null,null,null);e.default=component.exports},254:function(t,e,n){t.exports=n.p+"img/avatar4.cfa4c9c.jpg"},255:function(t,e,n){"use strict";n(246)},256:function(t,e,n){(e=n(25)(!1)).push([t.i,".social-link{margin-left:.75rem;margin-right:.75rem;font-size:1.5rem;--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}",""]),t.exports=e},272:function(t,e,n){"use strict";n.r(e);n(33);var r=n(5),o=n(249),l=n(108),c=n(240),m={},d=n(11),h=Object(d.a)(m,(function(){var t=this.$createElement;this._self._c;return this._m(0)}),[function(){var t=this.$createElement,e=this._self._c||t;return e("div",{staticClass:"wrapper-small text-center my-5"},[e("div",{staticClass:"text-4xl md:text-5xl text-gray-700 text-center"},[e("i",{staticClass:"bx bxl-python mx-2 md:mx-4"}),this._v(" "),e("i",{staticClass:"bx bxl-github mx-2 md:mx-4"}),this._v(" "),e("i",{staticClass:"bx bxs-coffee mx-2 md:mx-4"})])])}],!1,null,null,null).exports,x=n(250),f=n(251),v={components:{Hero:o.default,Social:l.default,Projects:c.default,GoodThings:h,MeDoThings:x.default,Affiliations:f.default},methods:{asyncData:function(t){return Object(r.a)(regeneratorRuntime.mark((function e(){var n,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return n=t.$content,e.next=3,n("blog").fetch();case 3:return r=e.sent,e.abrupt("return",{posts:r});case 5:case"end":return e.stop()}}),e)})))()}},head:function(){return{script:[{src:"https://identity.netlify.com/v1/netlify-identity-widget.js"}]}}},y=Object(d.a)(v,(function(){var t=this.$createElement,e=this._self._c||t;return e("div",{staticClass:"wrapper-small md:px-10"},[e("div",{staticClass:"header mt-5"},[e("Hero"),this._v(" "),e("MeDoThings"),this._v(" "),this._m(0),this._v(" "),e("Affiliations")],1)])}),[function(){var t=this.$createElement,e=this._self._c||t;return e("div",{staticClass:"pt-0"},[e("div",{staticClass:"flex justify-center items-center text-base font-semibold text-gray-600"},[e("h4",{staticClass:"text-center"},[this._v("Affiliations")]),this._v(" "),e("i",{staticClass:"bx bx-chevrons-down ml-1 mt-1"})])])}],!1,null,null,null);e.default=y.exports;installComponents(y,{Hero:n(249).default,MeDoThings:n(250).default,Affiliations:n(251).default})}}]);